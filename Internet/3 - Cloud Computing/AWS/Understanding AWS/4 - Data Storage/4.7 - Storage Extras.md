
### Snow Family: offline devices to process data at the edge and migrate data into and out of AWS
If it takes more than a week to transfer over the network, use Snowball devices **(physical route, not network route)**.

Highly-secure, portable devices to 
- either collect and process data at the edge
- or migrate data into and out of AWS

- Data migration
	- Snowcone
		- Use Snowcone where Snowball doesn't fit (space-constrained env)
		- Must provide your own battery / cables
		- Can be sent back to AWS offline, or connect it to internet and use AWS DataSync to send data
	- **Snowball Edge**
		- AWS Snowball Edge is a **physical data transfer device** designed for **large-scale data migrations** without consuming excessive network bandwidth. It allows for the **offline transfer** of large amounts of data from on-premises locations to AWS.
		- Comes with **computing capabilities** and allows you to pre-process the data while it's being moved into Snowball.
		- Use cases: large data cloud migrations, DC decommissions, disaster recovery 
	- Snowmobile: an actual truck
		- Better use case than Snowball if transfer more than 10PB
![[Snow-family-for-data-migrations.png]]

- Usage process
	- Request Snowball devices from the AWS console for delivery
	- Install the snowball client / AWS OpsHub on your servers
	- Connect the snowball to your servers and copy files using the client
	- Ship back the device when you’re done (goes to the right AWS facility)
	- Data will be loaded into an S3 bucket
	- Snowball is completely wiped

- Edge computing
	- Process data while it’s being created on an edge location
	- These locations may have
		- **Limited / no internet access**
		- **Limited / no easy access to computing power**
	- We setup a **Snowball Edge / Snowcone device** to do edge computing
		- Snowcone & Snowcone SSD (smaller)  
			- 2 CPUs, 4 GB of memory, wired or wireless access • USB-C power using a cord or the optional battery
		- Snowball Edge – **Compute Optimized**
			- 104 vCPUs, 416 GiB of RAM
			- Optional GPU **(useful for video processing or machine learning)**
			- 28 TB NVMe or 42TB HDD usable storage  
			- Storage Clustering available (up to 16 nodes)
		- Snowball Edge – Storage Optimized  
			- Up to 40 vCPUs, 80 GiB of RAM, 80TB storage
		- All: **Can run EC2 Instances & AWS Lambda functions (using AWS IoT Greengrass)** 
		- Long-term deployment options: **1 and 3 years discounted pricing**
	- Use cases
		- Preprocess data
		- Machine learning at the edge
		- Transcoding media streams
	- Eventually (if need be) we can ship back the device to AWS (for transferring data for example)
	- AWS OpsHub: a CLI (Command Line Interface tool) to use Snow Family devices
		- Unlocking and configuring single or clustered devices
		- Transferring files
		- Launching and managing instances running on Snow Family Devices
		- Monitor device metrics (storage capacity, active instances on your device)
		- Launch compatible AWS services on your devices (ex: Amazon EC2 instances, AWS DataSync, Network File System (NFS))

### Architecture: Snowball into Glacier
- Snowball cannot import to Glacier directly  
- You must **use Amazon S3 first**, in combination with an S3 lifecycle policy
![[snowball-into-glacier.png]]

### Amazon FSx
**fully managed service** to launch 3rd party high-performance file systems on AWS
- FSx for Windows
	- FSx for Windows is **a fully managed Windows file system share drive**
	- Suppor ts SMB protocol & Windows NTFS  
	- **Microsoft Active Directory integration, ACLs, userquotas**
	- has integration with Microsoft **Active Directory** for access control
	- Can be mounted on Linux EC2 instances
	- Supports Microsoft's **Distributed File System (DFS)** Namespaces (group files across multiple FS)
	- Scale up to 10s of GB/s, millions of IOPS, 100s PB of data
	- Storage Options:  
		- SSD – latency sensitive workloads (db, media processing, data analytics, ...) 
		- HDD – broad spectrum of workloads (home directory, CMS, ...)
	- **Can be accessed from your on-premises infrastructure (VPN or Direct Connect)**
	- Can be configured to be **Multi-AZ (high availability) ** 
	- **Data is backed-up daily to S3**

- FSx for Lustre
	- Lustre is a type of **parallel distributed file system, for large-scale computing**
	- The name Lustre is derived from **“Linux” and “cluster"**
	- **Machine Learning, High Performance Computing (HPC)**
	- **Video Processing, Financial Modeling, Electronic Design Automation**
	- Scales up to 100s GB/s, millions of IOPS, sub-ms latencies
	- Storage Options:  
		- SSD – low-latency, IOPS intensive workloads, small & random file operations • HDD – throughput-intensive workloads, large & sequential file operations
	- Seamless integration with S3
	- Can “read S3” as a file system (through FSx) 
	- Can write the output of the computations back to S3 (through FSx)
	- Can be used from on-premises servers (VPN or Direct Connect)
	- File System Deployment Options
		- Scratch File System
			- Temporary storage
			- Data is not replicated (doesn’t persist if file server fails)
			- High burst (6x faster, 200MBps per TiB)
			- Usage: short-term processing, optimize costs
		- Persistent File System
			- Long-term storage
			- Data is replicated within same AZ
			- Replace failed files within minutes
			- Usage: long-term processing, sensitive data

- FSx for NetApp ONTAP
	- Managed NetApp ONTAP on AWS
	- File System compatible with **NFS, SMB, iSCSI protocol**
	- Move workloads running on ONTAP or NAS to AWS
	- Works with: 
		- Linux  
	    - Windows
		- MacOS
	    - VMware Cloud on AWS
	    - Amazon Workspaces & AppStream 2.0
	    - Amazon EC2, ECS and EKS
	- Storage shrinks or grows automatically
	- Snapshots, replication, low-cost, compression and data
	- Point-in-time instantaneous cloning (helpful for testing new workloads)

- Amazon FSx for Tape Gateway
	- Managed NetApp ONTAP on AWS
	- File System compatible with **NFS, SMB, iSCSI protocol**
	- Move workloads running on ONTAP or NAS to AWS
	- Works with:
		- Linux
	    - Windows
	    - MacOS
	    - VMware Cloud on AWS
	    - Amazon Workspaces & AppStream 2.0
	    - Amazon EC2, ECS and EKS
	- Storage shrinks or grows automatically
	- Snapshots, replication, low-cost, compression and data
	- Point-in-time instantaneous cloning (helpful for testing new workloads)

### Storage Gateway
- Hybrid Cloud for Storage
	- AWS is pushing for ”hybrid cloud”  
	    - Part of your infrastructure is on the cloud
	    - Part of your infrastructure is on-premises
	- This can be due to  
	    - Long cloud migrations  
	    - Security requirements  
	    - Compliance requirements
	    - IT strategy
	- S3 is a **proprietary storage technology (unlike EFS / NFS)**, so how do you expose the S3 data on-premises?
	- AWS Storage Gateway!
![[AWS-strorage-cloud-native-options.png]]
- AWS Storage Gateway
	- Bridge between on-premises data and cloud data
	- Use cases:  
		- disaster recovery
		- backup & restore  
		- tiered storage  
		- on-premises cache & low-latency files access
	- Types of Storage Gateway:
		- S3 File Gateway
			- Configured S3 buckets are accessible using the NFS and SMB protocol  
			- Most recently used data is cached in the file gateway  
			- SupportsS3Standard, S3StandardIA, S3OneZoneA, S3IntelligentTiering  
			- **Transition to S3 Glacier using a *Lifecycle Policy***
			- Bucket access using IAM roles for each File Gateway  
			- SMB Protocol has integration with **Active Directory (AD)** for user authentication
		- FSx File Gateway
			- Native access to Amazon FSx for Windows File Server  
			- **Local cache** for frequently accessed data  
			- Windows native compatibility (SMB, NTFS, Active Directory...)
			- Useful for group file shares and home directories
		- Volume Gateway
			- Block storage using iSCSI protocol backed by S3  
			- Backed by EBS snapshots which can help restore on-premises volumes!
			- Cached volumes: low latency access to most recent data  
			- Stored volumes: entire dataset is on premise, scheduled backups to S3
		- Tape Gateway
			- Some companies have backup processes using physical tapes (!)  
			- With Tape Gateway, companies use the same processes but, in the cloud 
			- VirtualTape Library (VTL) backed by Amazon S3 and Glacier  
			- Back up data using existing tape-based processes (and iSCSI interface)  
			- Works with leading backup software vendors

Storage Gateway – Hardware appliance
- Using Storage Gateway means you need on-premises virtualization
- Otherwise, you can use a Storage Gateway Hardware Appliance
- You can buy it on amazon.com
- Works with File Gateway,Volume Gateway,
- Has the required CPU, memory, network, SSD cache resources
- Helpful for daily NFS backups in small data centers

![[AWS-storage-gateway.png]]

### Transfer family
- **A fully-managed service** for file transfers **into and out of S3 or EFS** using **FTP protocol**
- Supported Protocols  
    - AWS Transfer for **FTP (File Transfer Protocol (FTP)) ** 
    - AWS Transfer for **FTPS (File Transfer Protocol over SSL (FTPS))**
    - AWS Transfer for **SFTP (Secure File Transfer Protocol (SFTP))**
- Managed infrastructure, Scalable, Reliable, Highly Available (multi-AZ)
- Pay per provisioned endpoint per hour + data transfers in GB
- Store and manage users’ credentials within the service
- **Integrate with existing authentication systems (Microsoft Active Directory, LDAP, Okta, Amazon Cognito, custom)**
- Usage: sharing files, public datasets, CRM, ERP, ...
![[AWS-transfer-family.png]]

### DataSync
- Move large amount of data to and from  
	- On-premises / other cloud to AWS (NFS, SMB, HDFS, S3 API...) – **needs agent**
	- AWS to AWS (different storage services) – no agent needed
- Can synchronize to:  
	- Amazon S3 (any storage classes – including Glacier)
	- Amazon EFS  
	- Amazon FSx (Windows, Lustre, NetApp, OpenZFS...)
- Replication tasks can be scheduled hourly, daily, weekly  
	- File permissions and metadata are preserved (NFS POSIX, SMB...)
	- One agent task can use 10 Gbps, can setup a bandwidth limit
	![[AWS-Datasync.png]]
- Want to use DataSync, but don't have the network capacity to do so
		=> use AWS Snowcone that comes with the DataSync agent pre-installed on it
		=> So run Snowcone on-premises, pull the data, run DataSync agents, then it will be shipped back into AWS region
	 ![[AWS-DataSync-between-aws-resources.png]]